# Huffman Coding Compression

This project implements the **Huffman Coding** algorithm for lossless data compression and includes a comprehensive test suite (`test.py`) to compare its performance against **Lemppel-Ziv-Welch(LZW)** across various data distributions.

| File Name | Description |
| :--- | :--- |
| `huffman.py` | Core implementation of the Huffman tree building, encoding, and **decompression** logic. |
| `test.py` | The main test driver. It executes the compression, calculates performance ratios, and runs integrity checks. |
| `createequal.py` | Script to generate the `equal_dist.txt` file (data with a uniform pattern). |
| `createrandom.py` | Script to generate the `random.txt` file (data with random, uniform distribution). |
| `lzw.py` | Implementation of the LZW (Lempel-Ziv-Welch) algorithm. Serves as the "Competitor" algorithm to evaluate dictionary-based compression vs. Huffman. |
| `fixedlength.py` | Implementation of Fixed-Length (ASCII) encoding. Serves as the "Baseline" (Control Group) to calculate raw compression ratios. |
| `entropy.py` | Computes the Shannon entropy of a given text by analyzing character frequency and applying the formula H = −Σ p log₂(p). Serves as a theoretical benchmark for evaluating compression efficiency. |
| `analyze_entropy.py` | Script that reads input datasets (e.g., equal, random, Wikipedia text) and outputs their entropy values using `entropy.py`. Used to compare theoretical entropy with actual compression ratios across algorithms. |

---

### Step 1: Generate Input Files

The tests uses four test cases in line 34 of test.py:     
```
files = ["wikipedia.txt", "random.txt", "equal_dist.txt", "single_char.txt"]
```

1.  **Generate Specialized Files:**
    Run the following commands in your terminal or command prompt (in the project directory) to create the files: 
    ```bash
    python createrandom.py
    python createequal.py
    ```

2.  **Create `wikipedia.txt` (Manual Step):**
    Create a new file named `wikipedia.txt` in the project directory. Paste a large block of typical, **plain English text** (e.g., an article from Wikipedia) into this file. This represents data where character frequencies are highly uneven, the **ideal case for Huffman Coding**.

    *(Note: The `single_char.txt` file is automatically generated by `test.py`.)*

### Step 2: Run the Test

Execute the main test script from your terminal:

```bash
python test.py
```

#### Output includes: 
- Compression Ratios: Comparing Huffman vs. LZW vs. Fixed-Length.
- Theoretical Limits: Comparison against Shannon Entropy.
- Scalability Analysis: Testing performance on files of increasing size (10k $\to$ 100k).
