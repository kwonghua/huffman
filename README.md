# Huffman Coding Compression

This project implements the **Huffman Coding** algorithm for lossless data compression and includes a comprehensive test suite (`test.py`) to compare its performance against **Run-Length Encoding (RLE)** across various data distributions.

| File Name | Description |
| :--- | :--- |
| `huffman.py` | Core implementation of the Huffman tree building, encoding, and **decompression** logic. |
| `test.py` | The main test driver. It executes the compression, calculates performance ratios, and runs integrity checks. |
| `createequal.py` | Script to generate the `equal_dist.txt` file (data with a uniform pattern). |
| `createrandom.py` | Script to generate the `random.txt` file (data with random, uniform distribution). |

---

### Step 1: Generate Input Files

The tests uses four test cases in line 34 of test.py:     
```
files = ["wikipedia.txt", "random.txt", "equal_dist.txt", "single_char.txt"]
```

1.  **Generate Specialized Files:**
    Run the following commands in your terminal or command prompt (in the project directory) to create the files: 
    ```bash
    python createrandom.py
    python createequal.py
    ```

2.  **Create `wikipedia.txt` (Manual Step):**
    Create a new file named `wikipedia.txt` in the project directory. Paste a large block of typical, **plain English text** (e.g., an article from Wikipedia) into this file. This represents data where character frequencies are highly uneven, the **ideal case for Huffman Coding**.

    *(Note: The `single_char.txt` file is automatically generated by `test.py`.)*

### Step 2: Run the Test

Execute the main test script from your terminal:

```bash
python test.py
